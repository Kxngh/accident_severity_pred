{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunksize = 100000\n",
    "cleaned_file_path = 'cleaned_dataset.csv'\n",
    "is_first_chunk = True\n",
    "for chunk in pd.read_csv('US_Accidents_March23.csv', chunksize=chunksize):\n",
    "    chunk.drop(columns=['End_Lat', 'End_Lng'], inplace=True)\n",
    "    num_cols = ['Wind_Chill(F)', 'Precipitation(in)']\n",
    "    for col in num_cols:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col].fillna(chunk[col].mean(), inplace=True)\n",
    "    chunk['Start_Time'] = pd.to_datetime(chunk['Start_Time'], errors='coerce')\n",
    "    chunk['End_Time'] = pd.to_datetime(chunk['End_Time'], errors='coerce')\n",
    "    chunk['Duration_minutes'] = (chunk['End_Time'] - chunk['Start_Time']).dt.total_seconds() / 60\n",
    "    numerical_cols = ['Temperature(F)', 'Distance(mi)', 'Visibility(mi)', 'Wind_Speed(mph)']\n",
    "    for col in numerical_cols:\n",
    "        if col in chunk.columns:\n",
    "            chunk = chunk[chunk[col].between(chunk[col].quantile(0.01), chunk[col].quantile(0.99))]\n",
    "    chunk['Weather_Condition'] = chunk['Weather_Condition'].astype('category').cat.codes\n",
    "    chunk['Sunrise_Sunset'] = chunk['Sunrise_Sunset'].map({'Day': 1, 'Night': 0})\n",
    "    chunk['Civil_Twilight'] = chunk['Civil_Twilight'].map({'Day': 1, 'Night': 0})\n",
    "    chunk['Nautical_Twilight'] = chunk['Nautical_Twilight'].map({'Day': 1, 'Night': 0})\n",
    "    chunk['Astronomical_Twilight'] = chunk['Astronomical_Twilight'].map({'Day': 1, 'Night': 0})\n",
    "    chunk.dropna(inplace=True)\n",
    "    for col in chunk.select_dtypes(include='object').columns:\n",
    "        chunk[col] = chunk[col].astype('category')\n",
    "    chunk.to_csv(cleaned_file_path, mode='w' if is_first_chunk else 'a', index=False, header=is_first_chunk)\n",
    "    is_first_chunk = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_file_path = 'cleaned_dataset.csv'\n",
    "\n",
    "# Load the cleaned dataset for splitting\n",
    "data = pd.read_csv(cleaned_file_path)\n",
    "\n",
    "# Define the target column and features\n",
    "target = 'Severity'\n",
    "features = data.drop(columns=[target]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22552\\2741575721.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load the cleaned dataset for splitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Define the target column and features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned dataset for splitting\n",
    "data = pd.read_csv(cleaned_file_path)\n",
    "\n",
    "# Define the target column and features\n",
    "target = 'Severity'\n",
    "features = data.drop(columns=[target]).columns\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(data, test_size=0.25, random_state=42, stratify=data[target])\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.4, random_state=42, stratify=temp_data[target])\n",
    "\n",
    "# Save the splits to separate CSV files\n",
    "train_data.to_csv('train_dataset.csv', index=False)\n",
    "validation_data.to_csv('validation_dataset.csv', index=False)\n",
    "test_data.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "print(\"Data splitting completed. Train, validation, and test datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics:\n",
      "Accuracy: 0.8708\n",
      "Precision: 0.8599\n",
      "Recall: 0.8708\n",
      "F1 Score: 0.8591\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.37      0.51      9307\n",
      "           2       0.89      0.96      0.92    811052\n",
      "           3       0.76      0.53      0.63    160878\n",
      "           4       0.54      0.27      0.36     24488\n",
      "\n",
      "    accuracy                           0.87   1005725\n",
      "   macro avg       0.75      0.53      0.60   1005725\n",
      "weighted avg       0.86      0.87      0.86   1005725\n",
      "\n",
      "Model saved as random_forest_model.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Select only numeric columns for features\n",
    "features = data.select_dtypes(include=['float64', 'int64']).columns.drop(target)\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_validation = validation_data[features]\n",
    "y_validation = validation_data[target]\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "validation_predictions = model.predict(X_validation)\n",
    "accuracy = accuracy_score(y_validation, validation_predictions)\n",
    "precision = precision_score(y_validation, validation_predictions, average='weighted')\n",
    "recall = recall_score(y_validation, validation_predictions, average='weighted')\n",
    "f1 = f1_score(y_validation, validation_predictions, average='weighted')\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_validation, validation_predictions))\n",
    "\n",
    "joblib.dump(model, 'random_forest_model.joblib')\n",
    "print(\"Model saved as random_forest_model.joblib\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "Accuracy: 0.8708\n",
      "Precision: 0.8600\n",
      "Recall: 0.8708\n",
      "F1 Score: 0.8592\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.36      0.50      6204\n",
      "           2       0.89      0.96      0.92    540702\n",
      "           3       0.76      0.53      0.63    107252\n",
      "           4       0.54      0.27      0.36     16326\n",
      "\n",
      "    accuracy                           0.87    670484\n",
      "   macro avg       0.75      0.53      0.60    670484\n",
      "weighted avg       0.86      0.87      0.86    670484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_precision = precision_score(y_test, test_predictions, average='weighted')\n",
    "test_recall = recall_score(y_test, test_predictions, average='weighted')\n",
    "test_f1 = f1_score(y_test, test_predictions, average='weighted')\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19960\\3715305336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfeature_importances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msorted_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "feature_names = features\n",
    "sorted_idx = feature_importances.argsort()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
